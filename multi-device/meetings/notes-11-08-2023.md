# 11-08-2023

## Attendees

- Kevin (NVIDIA)
- Rama (Microsoft)
- Wei-Sheng (Microsoft)
- Yasushi (IBM)

## Recording
- [Recording Link](https://lists.lfaidata.foundation/g/onnx-wg-multidevice/files/Meeting%20Recordings)

## Agenda

- Shortening meeting cadence (2 weeks instead of one month)
- Defining requirements
- Discussion on single model vs multi-model representation
- Any other discussions


## Meeting Notes

### Requirements
What should we be able to represent?
* Model parallelism
* Tensor parallelism
* Data parallelism (not too relevant in inference usecase)
* Multiple devices across multiple nodes
* Ability to define subgroups with devices

What operators do we need to accomplish the above?
* Model parallelism
  - TensorSend  
  - TensorRecv
* Tensor parallelism
  - TensorScatter
  - TensorAllGather
  - TensorAllReduce (specialized on reduce operation)
  - TensorAllScatterReduce (composed as a function)
* How do initializers / weights of other operators get tiled?
* Sharding vs Reshuffling spec of tiling
* How expressive of a sharding pattern do we want to support?
  - LLAMA2 implementation only requires sharding across one axis
https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/contrib_ops/cuda/collective/sharding_spec.h 
  - General sharding spec can make current operations unrepresentable with current ONNX primitives
  - General sharding spec provides undue pressure on backends to support potentially uncommon sharding
* Distributed versions of primitives from ONNXRT: https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/test/python/onnxruntime_test_distributed.py 

Who produces these models?
* Nothing stopping upstream framework to insert collective ops at certain points
* Most likely third-party optimization tools will insert this based on target hardware domain

### Single model vs Multi-model representation

* Single model -  every device executes subset of nodes 
* Single model - every device executes the same model depending on device ID
* Multi-model - every device executes its own model

Targeting # 2 and # 3:

Base ONNX representation:

`Input -> Add -> Output`

Single model representation:

`Input(2x2) -> TensorScatter(axis 0 across two devices) -> Add (2x2) -> AllGather -> Output`

Multi model representation:

Device 0:
`input(2x2) -> TensorScatter(axis 0 across two devices) -> Add (1x2) -> AllGather -> Output`

Device 1:

`TensorScatter(some way to represent a recv from device 0) -> Add(1x2) -> AllGather(some way to represent sending back to device0)`

### Questions
* Do we assume some pre-determined machine configuration when generating the ONNX?
  - In ORT LLAMA implementation, we cannot change number of devices for memory concerns.
  - The optimized implementation needs to know the number of devices
  - Use case - generate a model with N number of devices at build time or runtime?
* Single model vs multi-model
  - If axis 0 is dynamic - some shape analysis is required
  - If the subgraph between TensorScatter & AllGather is large, how do we represent that?
  - Depends on the well-formed definition of collective operations


## Action Items

- Review ORT example, come up with definitions for collective operations, more concrete examples.
